{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.backends\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from agents import Agent, attach_agents\n",
    "from models import VGG16\n",
    "from training_utils import validate, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    checkpoint = 'best.pth.tar'  # Pretrained VGG16 weights for CIFAR-10.\n",
    "    num_workers = 0\n",
    "    batch_size = 256\n",
    "    lr_agents = 0.01\n",
    "    lr_model = 0.001\n",
    "    # epochs = 200 \n",
    "    penalty = 50  # lambda\n",
    "    init_weight = 6.9  # Agent's initial weight value.\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10('./data', train=True, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.RandomHorizontalFlip(),\n",
    "                                  transforms.RandomCrop(32, padding=4),\n",
    "                                  transforms.RandomRotation(20),\n",
    "                                  transforms.ToTensor(),\n",
    "                              ]))\n",
    "\n",
    "val_data = datasets.CIFAR10('./data', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                            ]))\n",
    "\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=args.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=args.num_workers)\n",
    "\n",
    "val_loader = DataLoader(val_data,\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=args.num_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(args.checkpoint, map_location=args.device)\n",
    "model = VGG16()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(args.device)\n",
    "model.eval();\n",
    "\n",
    "model_orig = copy.deepcopy(model)  # Copy of the original model, just in case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val: 100%|██████████| 40/40 [00:02<00:00, 17.73it/s, acc=0.919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.9194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_acc = validate(model, val_loader, args.device)\n",
    "print('Baseline accuracy: {}'.format(val_acc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_prune = [['features.0', 'features.1'],\n",
    "                    ['features.3', 'features.4'],\n",
    "                    ['features.7', 'features.8'],\n",
    "                    ['features.10', 'features.11'],\n",
    "                    ['features.14', 'features.15'],\n",
    "                    ['features.17', 'features.18'],\n",
    "                    ['features.20', 'features.21'],\n",
    "                    ['features.24', 'features.25'],\n",
    "                    ['features.27', 'features.28'],\n",
    "                    ['features.30', 'features.31'],\n",
    "                    ['features.34', 'features.35'],\n",
    "                    ['features.37', 'features.38'],\n",
    "                    ['features.40', 'features.41'],\n",
    "                    ['classifier.0', 'classifier.1']]\n",
    "\n",
    "agents, name_to_agent, num_agents, num_subagents = attach_agents(model,\n",
    "                                                                 modules_to_prune,\n",
    "                                                                 args.device,\n",
    "                                                                 args.init_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total agents: 4736\n"
     ]
    }
   ],
   "source": [
    "print('Total agents: {}'.format(sum(agent.num_subagents for agent in agents)))\n",
    "assert num_subagents == sum(agent.num_subagents for agent in agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val: 100%|██████████| 40/40 [00:02<00:00, 16.48it/s, acc=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 196/196 [00:46<00:00,  4.23it/s, acc=0.993, dropped=530, loss_a=0.102, loss_m=8.97e-5, p=0.859, p_max=1, p_min=0.00075, w=3.78] \n",
      "Val: 100%|██████████| 40/40 [00:02<00:00, 16.50it/s, acc=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 196/196 [00:46<00:00,  4.24it/s, acc=0.993, dropped=530, loss_a=0.103, loss_m=8.64e-5, p=0.859, p_max=1, p_min=0.00075, w=3.78]\n",
      "Val: 100%|██████████| 40/40 [00:02<00:00, 16.53it/s, acc=0.917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 196/196 [00:46<00:00,  4.23it/s, acc=0.994, dropped=530, loss_a=0.115, loss_m=6.87e-5, p=0.859, p_max=1, p_min=0.00075, w=3.78]\n",
      "Val: 100%|██████████| 40/40 [00:02<00:00, 16.25it/s, acc=0.915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 196/196 [00:46<00:00,  4.23it/s, acc=0.993, dropped=530, loss_a=0.105, loss_m=8.07e-5, p=0.859, p_max=1, p_min=0.00075, w=3.78]\n",
      "Val: 100%|██████████| 40/40 [00:02<00:00, 16.55it/s, acc=0.915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.99304,\n",
      " 'accuracy_reward_avg': 0.64504,\n",
      " 'compression_reward_avg': 60.0,\n",
      " 'full_reward_avg': 38.7024,\n",
      " 'loss_agents_avg': 0.10515757954597472,\n",
      " 'loss_model_avg': 8.068947711959481e-05,\n",
      " 'num_channels_dropped': 530.0,\n",
      " 'p_max': 0.9999513626098633,\n",
      " 'p_min': 0.0007500865031033754,\n",
      " 'probabilities_avg': 0.8589694457394736,\n",
      " 'total_channels': 4736,\n",
      " 'total_samples': 50000,\n",
      " 'weights_avg': 3.779762898172651}\n",
      "Val accuracy: 0.9148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer_agents = optim.Adam([agent.w for agent in agents], lr=args.lr_agents)\n",
    "# scheduler_agents = StepLR(optimizer_agents, step_size=5, gamma=0.3)\n",
    "\n",
    "optimizer_model = optim.Adam(model.parameters(), lr=args.lr_model)\n",
    "# scheduler_model = StepLR(optimizer_model, step_size=5, gamma=0.3)\n",
    "\n",
    "criterion_model = nn.CrossEntropyLoss().to(args.device)\n",
    "\n",
    "# Joint training of policies and the model.\n",
    "for epoch in range(260):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "\n",
    "    train_logs = train(model, agents, train_loader, optimizer_model,\n",
    "                       optimizer_agents, criterion_model, args.penalty,\n",
    "                       args.device, optimize_agents=True, optimize_model=True)\n",
    "\n",
    "    [agent.eval() for agent in agents]\n",
    "    val_acc = validate(model, val_loader, args.device)\n",
    "\n",
    "pprint.pprint(train_logs)\n",
    "print('Val accuracy: {}'.format(val_acc))\n",
    "\n",
    "# Fine-tuning the model. Stopped policy training.\n",
    "print('-'*100)\n",
    "print('Fine-tuning...')\n",
    "\n",
    "p_list = []\n",
    "for agent in agents:\n",
    "    p_list += torch.sigmoid(agent.w).tolist()    \n",
    "print('Agents p <= 0.5: {}'.format(sum(p <= 0.5 for p in p_list)))\n",
    "\n",
    "[agent.eval(prob_threshold=0.5, threshold_type='BINARY') for agent in agents]\n",
    "\n",
    "for epoch in range(260, 300):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "\n",
    "    train_logs = train(model, agents, train_loader, optimizer_model,\n",
    "                       optimizer_agents, criterion_model, args.penalty,\n",
    "                       args.device, optimize_agents=False, optimize_model=True)\n",
    "    val_acc = validate(model, val_loader, args.device)\n",
    "    \n",
    "pprint.pprint(train_logs)\n",
    "print('Val accuracy: {}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg p: 0.8644156797998074\n",
      "Min p: 0.0007500865031033754\n",
      "Max p: 0.9999513626098633\n",
      "Total agents: 4736\n",
      "Agents p <= 0.5: 530\n",
      "Agents p <= 0.9: 1173\n",
      "Agents p <= 0.998: 4127\n",
      "Agents p <= 0.999: 4467\n"
     ]
    }
   ],
   "source": [
    "p_list = []\n",
    "for agent in agents:\n",
    "    p_list += torch.sigmoid(agent.w).tolist()    \n",
    "\n",
    "print('Avg p: {}'.format(sum(p_list)/len(p_list)))\n",
    "print('Min p: {}'.format(min(p_list)))\n",
    "print('Max p: {}'.format(max(p_list)))\n",
    "print('Total agents: {}'.format(sum(agent.num_subagents for agent in agents)))\n",
    "print('Agents p <= 0.5: {}'.format(sum(p <= 0.5 for p in p_list)))\n",
    "print('Agents p <= 0.9: {}'.format(sum(p <= 0.9 for p in p_list)))\n",
    "print('Agents p <= 0.998: {}'.format(sum(p <= 0.998 for p in p_list)))\n",
    "print('Agents p <= 0.999: {}'.format(sum(p <= 0.999 for p in p_list)))\n",
    "\n",
    "assert len(p_list) == sum(agent.num_subagents for agent in agents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.compression.pytorch import ModelSpeedup\n",
    "from nni.compression.pytorch.utils import count_flops_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarath/Desktop/decore/agents.py:95: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525498485/work/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  self.masks[module_name]['weight'][torch.nonzero(a)] = 1\n"
     ]
    }
   ],
   "source": [
    "# Masks for pruning channels. \n",
    "\n",
    "masks = {}\n",
    "for agent in agents:\n",
    "    agent.eval(prob_threshold=0.5, threshold_type='BINARY')\n",
    "    masks.update(agent.get_masks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-12 23:14:34] \u001b[32mstart to speedup the model\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32minfer module masks...\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32mUpdate mask for features.0\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32mUpdate mask for features.1\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32mUpdate mask for features.2\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32mUpdate mask for features.3\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32mUpdate mask for features.4\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32mUpdate mask for features.5\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32mUpdate mask for features.6\u001b[0m\n",
      "[2023-03-12 23:14:34] \u001b[32mUpdate mask for features.7\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.8\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.9\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.10\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.11\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.12\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.13\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.14\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.15\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.16\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.17\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.18\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.19\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.20\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.21\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.22\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.23\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.24\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.25\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.26\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.27\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.28\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.29\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.30\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.31\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.32\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.33\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.34\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.35\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.36\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.37\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.38\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.39\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.40\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.41\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.42\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for features.43\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for .aten::flatten.48\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for classifier.0\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for classifier.1\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for classifier.2\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate mask for classifier.3\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the classifier.3\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the classifier.2\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the classifier.1\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the classifier.0\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the .aten::flatten.48\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.43\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.42\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.41\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.40\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.39\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.38\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.37\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.36\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.35\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.34\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.33\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.32\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.31\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.30\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.29\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.28\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.27\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.26\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.25\u001b[0m\n",
      "[2023-03-12 23:14:35] \u001b[32mUpdate the indirect sparsity for the features.24\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.23\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.22\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.21\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.20\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.19\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.18\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.17\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.16\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.15\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.14\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.13\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.12\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.11\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.10\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.9\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.8\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.7\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.6\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.5\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.4\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.3\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.2\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.1\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mUpdate the indirect sparsity for the features.0\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mresolve the mask conflict\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace compressed modules...\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 3, out_channels: 48\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 48\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 48, out_channels: 57\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 57\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 57, out_channels: 109\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 109\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 109, out_channels: 113\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 113\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 113, out_channels: 232\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 232\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 232, out_channels: 228\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 228\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 228, out_channels: 219\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 219\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 219, out_channels: 462\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 462\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 462, out_channels: 456\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 456\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 456, out_channels: 458\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 458\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 458, out_channels: 461\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 461\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace conv2d with in_channels: 461, out_channels: 453\u001b[0m\n",
      "[2023-03-12 23:14:36] \u001b[32mreplace batchnorm2d with num_features: 453\u001b[0m\n",
      "[2023-03-12 23:14:37] \u001b[32mreplace conv2d with in_channels: 453, out_channels: 458\u001b[0m\n",
      "[2023-03-12 23:14:37] \u001b[32mreplace batchnorm2d with num_features: 458\u001b[0m\n",
      "[2023-03-12 23:14:37] \u001b[32mreplace linear with new in_features: 458, out_features: 452\u001b[0m\n",
      "[2023-03-12 23:14:37] \u001b[32mreplace batchnorm1d with num_features: 452\u001b[0m\n",
      "[2023-03-12 23:14:37] \u001b[32mreplace linear with new in_features: 452, out_features: 10\u001b[0m\n",
      "[2023-03-12 23:14:37] \u001b[32mspeedup done\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_pruned = copy.deepcopy(model_orig)\n",
    "model_pruned.load_state_dict(model.state_dict());\n",
    "dummy_input = torch.ones((1, 3, 32, 32)).to(args.device)\n",
    "\n",
    "ModelSpeedup(model_pruned, dummy_input, masks).speedup_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+------------------+------------------+------------------+----------+---------+\n",
      "| Index | Name         |     Type    |   Weight Shape   |    Input Size    |   Output Size    |  FLOPs   | #Params |\n",
      "+-------+--------------+-------------+------------------+------------------+------------------+----------+---------+\n",
      "|   0   | features.0   |    Conv2d   |  (48, 3, 3, 3)   |  (1, 3, 32, 32)  | (1, 48, 32, 32)  | 1376256  |   1344  |\n",
      "|   1   | features.1   | BatchNorm2d |      (48,)       | (1, 48, 32, 32)  | (1, 48, 32, 32)  |  98304   |    96   |\n",
      "|   2   | features.3   |    Conv2d   |  (57, 48, 3, 3)  | (1, 48, 32, 32)  | (1, 57, 32, 32)  | 25273344 |  24681  |\n",
      "|   3   | features.4   | BatchNorm2d |      (57,)       | (1, 57, 32, 32)  | (1, 57, 32, 32)  |  116736  |   114   |\n",
      "|   4   | features.7   |    Conv2d   | (109, 57, 3, 3)  | (1, 57, 16, 16)  | (1, 109, 16, 16) | 14342656 |  56026  |\n",
      "|   5   | features.8   | BatchNorm2d |      (109,)      | (1, 109, 16, 16) | (1, 109, 16, 16) |  55808   |   218   |\n",
      "|   6   | features.10  |    Conv2d   | (113, 109, 3, 3) | (1, 109, 16, 16) | (1, 113, 16, 16) | 28407296 |  110966 |\n",
      "|   7   | features.11  | BatchNorm2d |      (113,)      | (1, 113, 16, 16) | (1, 113, 16, 16) |  57856   |   226   |\n",
      "|   8   | features.14  |    Conv2d   | (232, 113, 3, 3) |  (1, 113, 8, 8)  |  (1, 232, 8, 8)  | 15115264 |  236176 |\n",
      "|   9   | features.15  | BatchNorm2d |      (232,)      |  (1, 232, 8, 8)  |  (1, 232, 8, 8)  |  29696   |   464   |\n",
      "|   10  | features.17  |    Conv2d   | (228, 232, 3, 3) |  (1, 232, 8, 8)  |  (1, 228, 8, 8)  | 30482688 |  476292 |\n",
      "|   11  | features.18  | BatchNorm2d |      (228,)      |  (1, 228, 8, 8)  |  (1, 228, 8, 8)  |  29184   |   456   |\n",
      "|   12  | features.20  |    Conv2d   | (219, 228, 3, 3) |  (1, 228, 8, 8)  |  (1, 219, 8, 8)  | 28774848 |  449607 |\n",
      "|   13  | features.21  | BatchNorm2d |      (219,)      |  (1, 219, 8, 8)  |  (1, 219, 8, 8)  |  28032   |   438   |\n",
      "|   14  | features.24  |    Conv2d   | (462, 219, 3, 3) |  (1, 219, 4, 4)  |  (1, 462, 4, 4)  | 14577024 |  911064 |\n",
      "|   15  | features.25  | BatchNorm2d |      (462,)      |  (1, 462, 4, 4)  |  (1, 462, 4, 4)  |  14784   |   924   |\n",
      "|   16  | features.27  |    Conv2d   | (456, 462, 3, 3) |  (1, 462, 4, 4)  |  (1, 456, 4, 4)  | 30344064 | 1896504 |\n",
      "|   17  | features.28  | BatchNorm2d |      (456,)      |  (1, 456, 4, 4)  |  (1, 456, 4, 4)  |  14592   |   912   |\n",
      "|   18  | features.30  |    Conv2d   | (458, 456, 3, 3) |  (1, 456, 4, 4)  |  (1, 458, 4, 4)  | 30081440 | 1880090 |\n",
      "|   19  | features.31  | BatchNorm2d |      (458,)      |  (1, 458, 4, 4)  |  (1, 458, 4, 4)  |  14656   |   916   |\n",
      "|   20  | features.34  |    Conv2d   | (461, 458, 3, 3) |  (1, 458, 2, 2)  |  (1, 461, 2, 2)  | 7602812  | 1900703 |\n",
      "|   21  | features.35  | BatchNorm2d |      (461,)      |  (1, 461, 2, 2)  |  (1, 461, 2, 2)  |   3688   |   922   |\n",
      "|   22  | features.37  |    Conv2d   | (453, 461, 3, 3) |  (1, 461, 2, 2)  |  (1, 453, 2, 2)  | 7519800  | 1879950 |\n",
      "|   23  | features.38  | BatchNorm2d |      (453,)      |  (1, 453, 2, 2)  |  (1, 453, 2, 2)  |   3624   |   906   |\n",
      "|   24  | features.40  |    Conv2d   | (458, 453, 3, 3) |  (1, 453, 2, 2)  |  (1, 458, 2, 2)  | 7470896  | 1867724 |\n",
      "|   25  | features.41  | BatchNorm2d |      (458,)      |  (1, 458, 2, 2)  |  (1, 458, 2, 2)  |   3664   |   916   |\n",
      "|   26  | classifier.0 |    Linear   |    (452, 458)    |     (1, 458)     |     (1, 452)     |  207468  |  207468 |\n",
      "|   27  | classifier.1 | BatchNorm1d |      (452,)      |     (1, 452)     |     (1, 452)     |   904    |   904   |\n",
      "|   28  | classifier.3 |    Linear   |    (10, 452)     |     (1, 452)     |     (1, 10)      |   4530   |   4530  |\n",
      "+-------+--------------+-------------+------------------+------------------+------------------+----------+---------+\n",
      "FLOPs total: 242051914\n",
      "#Params total: 11911537\n"
     ]
    }
   ],
   "source": [
    "flops1, params1, results1 = count_flops_params(model_pruned, (1, 3, 32, 32), mode='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------+------------------+------------------+------------------+----------+---------+\n",
      "| Index | Name         |     Type    |   Weight Shape   |    Input Size    |   Output Size    |  FLOPs   | #Params |\n",
      "+-------+--------------+-------------+------------------+------------------+------------------+----------+---------+\n",
      "|   0   | features.0   |    Conv2d   |  (64, 3, 3, 3)   |  (1, 3, 32, 32)  | (1, 64, 32, 32)  | 1835008  |   1792  |\n",
      "|   1   | features.1   | BatchNorm2d |      (64,)       | (1, 64, 32, 32)  | (1, 64, 32, 32)  |  131072  |   128   |\n",
      "|   2   | features.3   |    Conv2d   |  (64, 64, 3, 3)  | (1, 64, 32, 32)  | (1, 64, 32, 32)  | 37814272 |  36928  |\n",
      "|   3   | features.4   | BatchNorm2d |      (64,)       | (1, 64, 32, 32)  | (1, 64, 32, 32)  |  131072  |   128   |\n",
      "|   4   | features.7   |    Conv2d   | (128, 64, 3, 3)  | (1, 64, 16, 16)  | (1, 128, 16, 16) | 18907136 |  73856  |\n",
      "|   5   | features.8   | BatchNorm2d |      (128,)      | (1, 128, 16, 16) | (1, 128, 16, 16) |  65536   |   256   |\n",
      "|   6   | features.10  |    Conv2d   | (128, 128, 3, 3) | (1, 128, 16, 16) | (1, 128, 16, 16) | 37781504 |  147584 |\n",
      "|   7   | features.11  | BatchNorm2d |      (128,)      | (1, 128, 16, 16) | (1, 128, 16, 16) |  65536   |   256   |\n",
      "|   8   | features.14  |    Conv2d   | (256, 128, 3, 3) |  (1, 128, 8, 8)  |  (1, 256, 8, 8)  | 18890752 |  295168 |\n",
      "|   9   | features.15  | BatchNorm2d |      (256,)      |  (1, 256, 8, 8)  |  (1, 256, 8, 8)  |  32768   |   512   |\n",
      "|   10  | features.17  |    Conv2d   | (256, 256, 3, 3) |  (1, 256, 8, 8)  |  (1, 256, 8, 8)  | 37765120 |  590080 |\n",
      "|   11  | features.18  | BatchNorm2d |      (256,)      |  (1, 256, 8, 8)  |  (1, 256, 8, 8)  |  32768   |   512   |\n",
      "|   12  | features.20  |    Conv2d   | (256, 256, 3, 3) |  (1, 256, 8, 8)  |  (1, 256, 8, 8)  | 37765120 |  590080 |\n",
      "|   13  | features.21  | BatchNorm2d |      (256,)      |  (1, 256, 8, 8)  |  (1, 256, 8, 8)  |  32768   |   512   |\n",
      "|   14  | features.24  |    Conv2d   | (512, 256, 3, 3) |  (1, 256, 4, 4)  |  (1, 512, 4, 4)  | 18882560 | 1180160 |\n",
      "|   15  | features.25  | BatchNorm2d |      (512,)      |  (1, 512, 4, 4)  |  (1, 512, 4, 4)  |  16384   |   1024  |\n",
      "|   16  | features.27  |    Conv2d   | (512, 512, 3, 3) |  (1, 512, 4, 4)  |  (1, 512, 4, 4)  | 37756928 | 2359808 |\n",
      "|   17  | features.28  | BatchNorm2d |      (512,)      |  (1, 512, 4, 4)  |  (1, 512, 4, 4)  |  16384   |   1024  |\n",
      "|   18  | features.30  |    Conv2d   | (512, 512, 3, 3) |  (1, 512, 4, 4)  |  (1, 512, 4, 4)  | 37756928 | 2359808 |\n",
      "|   19  | features.31  | BatchNorm2d |      (512,)      |  (1, 512, 4, 4)  |  (1, 512, 4, 4)  |  16384   |   1024  |\n",
      "|   20  | features.34  |    Conv2d   | (512, 512, 3, 3) |  (1, 512, 2, 2)  |  (1, 512, 2, 2)  | 9439232  | 2359808 |\n",
      "|   21  | features.35  | BatchNorm2d |      (512,)      |  (1, 512, 2, 2)  |  (1, 512, 2, 2)  |   4096   |   1024  |\n",
      "|   22  | features.37  |    Conv2d   | (512, 512, 3, 3) |  (1, 512, 2, 2)  |  (1, 512, 2, 2)  | 9439232  | 2359808 |\n",
      "|   23  | features.38  | BatchNorm2d |      (512,)      |  (1, 512, 2, 2)  |  (1, 512, 2, 2)  |   4096   |   1024  |\n",
      "|   24  | features.40  |    Conv2d   | (512, 512, 3, 3) |  (1, 512, 2, 2)  |  (1, 512, 2, 2)  | 9439232  | 2359808 |\n",
      "|   25  | features.41  | BatchNorm2d |      (512,)      |  (1, 512, 2, 2)  |  (1, 512, 2, 2)  |   4096   |   1024  |\n",
      "|   26  | classifier.0 |    Linear   |    (512, 512)    |     (1, 512)     |     (1, 512)     |  262656  |  262656 |\n",
      "|   27  | classifier.1 | BatchNorm1d |      (512,)      |     (1, 512)     |     (1, 512)     |   1024   |   1024  |\n",
      "|   28  | classifier.3 |    Linear   |    (10, 512)     |     (1, 512)     |     (1, 10)      |   5130   |   5130  |\n",
      "+-------+--------------+-------------+------------------+------------------+------------------+----------+---------+\n",
      "FLOPs total: 314294794\n",
      "#Params total: 14991946\n"
     ]
    }
   ],
   "source": [
    "flops2, params2, results2 = count_flops_params(model_orig, (1, 3, 32, 32), mode='full')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val: 100%|██████████| 40/40 [00:02<00:00, 19.09it/s, acc=0.915]\n",
      "Val: 100%|██████████| 40/40 [00:02<00:00, 16.69it/s, acc=0.915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned acc: 0.9148\n",
      "acc: 0.9148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pruned model\n",
    "val_acc_pruned = validate(model_pruned, val_loader, args.device)\n",
    "val_acc = validate(model, val_loader, args.device)\n",
    "print('Pruned acc: {}'.format(val_acc_pruned))\n",
    "print('acc: {}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penaly = -40\n",
    "# FLOPs total: 214681294\n",
    "# #Params total: 10671760\n",
    "\n",
    "# penalty = -4\n",
    "# FLOPs total: 43417836\n",
    "# #Params total: 2016099\n",
    "# Acc: 0.8941\n",
    "# PR FLOPs: 86.185633097\n",
    "# PR Params: 86.552119384\n",
    "\n",
    "# penalty = -50\n",
    "# FLOPs total: 242051914\n",
    "# #Params total: 11911537\n",
    "# Acc: 0.9148\n",
    "# PR FLOPs: 22.9857069\n",
    "# PR Params: 20.5470924\n",
    "\n",
    "# original\n",
    "# FLOPs total: 314294794\n",
    "# # Params total: 14991946\n",
    "# Acc: 0.9194"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd93ba6a5319cf2f6ba12746a6142d623d871224f9600524751428eb44288806"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
